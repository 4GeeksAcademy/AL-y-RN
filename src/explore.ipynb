{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
                "from scipy.stats import shapiro\n",
                "\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/decision-tree-project-tutorial/main/diabetes.csv\"\n",
                "df = pd.read_csv(url)\n",
                "df\n",
                "\n",
                "df.describe()\n",
                "\n",
                "df[\"BloodPressure\"].value_counts()\n",
                "\n",
                "df.isnull().sum()\n",
                "\n",
                "def percentage_nulls(df):\n",
                "    \"\"\"\n",
                "    This function returns a dictionary with the column and\n",
                "    the porcentage of missing values\n",
                "    \"\"\"\n",
                "    N_rows = df.shape[0]\n",
                "    vars_ = {}\n",
                "    for var in df.columns:\n",
                "        vars_[var]=(df[var].isnull().sum() / N_rows)\n",
                "    return vars_\n",
                "\n",
                "percentage_nulls(df)\n",
                "\n",
                "df=df.drop_duplicates().reset_index(drop = True) #eliminar duplicados\n",
                "\n",
                "df.columns[np.array(df.dtypes == \"object\")] #the non numeric columns es poco util porque no hay categoricas\n",
                "\n",
                "def binary(data):\n",
                "  bin_reg = bin_reg = r\"^[01](?:\\.0)?\\.?$\"\n",
                "  if str(data) == 'nan':\n",
                "    return np.nan\n",
                "  else:\n",
                "    return bool(re.findall(bin_reg, str(data)))\n",
                "\n",
                "def is_binary(df_, col):\n",
                "  \"\"\"\n",
                "  to consider this as a pure binary var take into account that\n",
                "  the others not reach a limit of range..\n",
                "  #no puedehaber un porcentaje de varriable superior con más de un  digito!!!!!!\n",
                "  \"\"\"\n",
                "  df = df_.copy()\n",
                "  percent =  df[col].apply(binary).sum() / df[col].count()\n",
                "  if percent > 0.5:\n",
                "    return True\n",
                "  else:\n",
                "    return False\n",
                "\n",
                "normal  = []\n",
                "nonormal = []\n",
                "binaries = []\n",
                "cates = []\n",
                "\n",
                "def tipo_var(df_):\n",
                "  df = df_.copy()\n",
                "  for col in df.columns:\n",
                "    if df[col].dtypes.name=='int64' or df[col].dtypes.name == 'float64':\n",
                "      if is_binary(df, col):\n",
                "        binaries.append(col)\n",
                "      else:\n",
                "        if shapiro(df[col]).pvalue > 0.05:\n",
                "          normal.append(col)\n",
                "        else:\n",
                "          nonormal.append(col)\n",
                "    else:\n",
                "      cates.append(col)\n",
                "  return normal, nonormal, binaries,  cates\n",
                "\n",
                "tipo_var(df) #el primero esta vacio es normales\n",
                "#el segundo es no normales son varias aqui\n",
                "#el tercero es binaria (target)\n",
                "#el ultimo son categoricas (aqui vacio)\n",
                "\n",
                "df['Outcome'].value_counts() #cuando hay que tomar accion y balancera datos?\n",
                "\n",
                "# Regex para seleccionar columnas numéricas\n",
                "numerical_columns = [col for col in df.columns if re.match(r'^[A-Za-z]+$', col) and col != 'Outcome'] #quito manualmente Target\n",
                "df[numerical_columns].describe()\n",
                "\n",
                "\n",
                "!pip install tableone\n",
                "\n",
                "\n",
                "#matriz de correlacion\n",
                "corr = df.corr(numeric_only=True) #importante si hay categoricos que este activo el parentesis\n",
                "corr\n",
                "\n",
                "sns.heatmap(corr, annot=True)\n",
                "\n",
                "from tableone import TableOne\n",
                "\n",
                "# Definir las columnas numéricas y categóricas (para numericas podria haber usado numerical_columns como ya estaba definida)\n",
                "columns = df.columns.drop('Outcome').tolist()\n",
                "categorical = []  # si tuviera variables categóricas van aquí\n",
                "\n",
                "# Crear TableOne\n",
                "table1 = TableOne(data=df, nonnormal=nonormal, categorical=categorical, groupby='Outcome', pval=True, htest_name=True) #divido entre con y sin diabetes\n",
                "\n",
                "table1\n",
                "\n",
                "\n",
                "#Separar el dataset en train y test\n",
                "X = df.drop(columns=['Outcome'])\n",
                "y = df['Outcome']\n",
                "\n",
                "# 2. Dividir en train/test antes de escalar\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
                "\n",
                "# 3. Escalar solo usando la media y std de X_train\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)  # transform SOLO (sin fit)\n",
                "\n",
                "#algoritmo para arboles de decision\n",
                "\n",
                "def grid_dt(X_train, y_train):\n",
                "    model = DecisionTreeClassifier(random_state=666)\n",
                "\n",
                "    class_weight = [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}]\n",
                "    max_depth = [None, 3, 5, 10] #cuidado si uso solo none es una tupla de un solo valor, no una lista o un valor único. agrego 3, 5, 10\n",
                "    min_samples_leaf = [5, 10, 20, 50, 100]\n",
                "    criterion  = [\"gini\", \"entropy\"]\n",
                "\n",
                "    grid = dict(\n",
                "        class_weight=class_weight,\n",
                "        max_depth=max_depth,\n",
                "        min_samples_leaf=min_samples_leaf,\n",
                "        criterion=criterion\n",
                "    )\n",
                "\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
                "\n",
                "    grid_search = GridSearchCV(\n",
                "        estimator=model,\n",
                "        param_grid=grid,\n",
                "        n_jobs=-1,\n",
                "        cv=cv,\n",
                "        scoring='f1',\n",
                "        error_score=0,\n",
                "        verbose=1\n",
                "    )\n",
                "\n",
                "    grid_result = grid_search.fit(X_train, y_train)\n",
                "    return grid_search.best_estimator_\n",
                "\n",
                "\n",
                "#entreno modelo arbol de decision\n",
                "best_dt = grid_dt(X_train_scaled, y_train)\n",
                "y_pred_dt = best_dt.predict(X_test_scaled)\n",
                "\n",
                "print(classification_report(y_test, y_pred_dt))\n",
                "\n",
                "\n",
                "#Ahora voy a comparar con regresion logistca\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
                "\n",
                "def grid_search_lr(X_train, y_train):\n",
                "    model = LogisticRegression(random_state=666, max_iter=1000)\n",
                "\n",
                "    class_weight = [{0:0.05, 1:0.95}, {0:0.1, 1:0.9}, {0:0.2, 1:0.8}]\n",
                "    solvers = ['liblinear']\n",
                "    penalty = ['l1', 'l2']\n",
                "    c_values = [10, 1.0, 0.1, 0.01, 0.001]\n",
                "\n",
                "    grid = dict(\n",
                "        solver=solvers,\n",
                "        penalty=penalty,\n",
                "        C=c_values,\n",
                "        class_weight=class_weight\n",
                "    )\n",
                "\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
                "\n",
                "    grid_search = GridSearchCV(\n",
                "        estimator=model,\n",
                "        param_grid=grid,\n",
                "        n_jobs=-1,\n",
                "        cv=cv,\n",
                "        scoring='f1',\n",
                "        error_score=0,\n",
                "        verbose=1\n",
                "    )\n",
                "\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    best_model = grid_search.best_estimator_\n",
                "    best_params = grid_search.best_params_\n",
                "\n",
                "    print(\"Mejores hiperparámetros:\", best_params)\n",
                "    return best_model\n",
                "\n",
                "\n",
                "#entreno modelo de regresion logistca\n",
                "best_lr = grid_search_lr(X_train_scaled, y_train)\n",
                "y_pred_lr = best_lr.predict(X_test_scaled)\n",
                "\n",
                "print(classification_report(y_test, y_pred_lr))\n",
                "\n",
                "\n",
                "#ahora voy a comparar con un modelo de redes neuronales\n",
                "\n",
                "def grid_search_mlp(X_train, y_train):\n",
                "    model = MLPClassifier(max_iter=1000, random_state=666)\n",
                "\n",
                "    hidden_layer_sizes = [(10,), (20,), (20, 10)]\n",
                "    activation = ['relu', 'tanh']\n",
                "    alpha = [0.0001, 0.001, 0.01]\n",
                "    learning_rate_init = [0.001, 0.01]\n",
                "    solver = ['adam']  # 'adam' suele ser más estable que 'sgd' sin mucha parametrización\n",
                "\n",
                "    param_grid = dict(\n",
                "        hidden_layer_sizes=hidden_layer_sizes,\n",
                "        activation=activation,\n",
                "        alpha=alpha,\n",
                "        learning_rate_init=learning_rate_init,\n",
                "        solver=solver\n",
                "    )\n",
                "\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
                "\n",
                "    grid_search = GridSearchCV(\n",
                "        estimator=model,\n",
                "        param_grid=param_grid,\n",
                "        scoring='f1',\n",
                "        n_jobs=-1,\n",
                "        cv=cv,\n",
                "        error_score=0,\n",
                "        verbose=1\n",
                "    )\n",
                "\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    best_model = grid_search.best_estimator_\n",
                "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
                "    return best_model\n",
                "\n",
                "\n",
                "#entreno modelo redes neuronales\n",
                "best_mlp = grid_search_mlp(X_train_scaled, y_train)\n",
                "y_pred_mlp = best_mlp.predict(X_test_scaled)\n",
                "\n",
                "print(classification_report(y_test, y_pred_mlp))\n",
                "\n",
                "\n",
                "! pip install imbalanced-learn > null\n",
                "\n",
                "# Undersampling si el ratio es mayor a 1:5\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "rus = RandomUnderSampler(random_state=1234)\n",
                "X_res, y_res = rus.fit_resample(X, y)\n",
                "X_res, y_res = rus.fit_resample(X, y)\n",
                "\n",
                "def classification_report_to_df(y_true, y_pred):\n",
                "    report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
                "    df_report = pd.DataFrame(report_dict).transpose()\n",
                "    df_report = df_report.drop(index=['accuracy', 'macro avg', 'weighted avg'])  # Opcional\n",
                "    return df_report\n",
                "\n",
                "#grafico comparativo de metricas de los clasificadores\n",
                "reportmlp = classification_report(y_test, y_pred_mlp, output_dict=True)\n",
                "reportlr = classification_report(y_test, y_pred_lr, output_dict=True)\n",
                "reportdt = classification_report(y_test, y_pred_dt, output_dict=True)\n",
                "\n",
                "[[reportdt['0']['f1-score'],reportdt['1']['f1-score'],reportdt['accuracy']],\n",
                "   [reportlr['0']['f1-score'],reportlr['1']['f1-score'],reportlr['accuracy']],\n",
                "   [reportmlp['0']['f1-score'],reportmlp['1']['f1-score'],reportmlp['accuracy']]]\n",
                "\n",
                "report_val=np.transpose([[reportdt['0']['f1-score'],reportdt['1']['f1-score'],reportdt['accuracy']],\n",
                "   [reportlr['0']['f1-score'],reportlr['1']['f1-score'],reportlr['accuracy']],\n",
                "   [reportmlp['0']['f1-score'],reportmlp['1']['f1-score'],reportmlp['accuracy']]])\n",
                "report_val\n",
                "\n",
                "classificador = (\"MLP\", \"logistric reg\", \"decision tree\")\n",
                "classification_metrics = {\n",
                "    '0 - f1score': report_val[0],\n",
                "    '1 - f1score': report_val[1],\n",
                "    'accuracy': report_val[2],\n",
                "}\n",
                "\n",
                "x = np.arange(len(classificador))  # the label locations\n",
                "width = 0.25  # the width of the bars\n",
                "multiplier = 0\n",
                "\n",
                "fig, ax = plt.subplots(layout='constrained')\n",
                "\n",
                "for attribute, measurement in classification_metrics.items():\n",
                "    offset = width * multiplier\n",
                "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
                "    ax.bar_label(rects, padding=3)\n",
                "    multiplier += 1\n",
                "\n",
                "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
                "ax.set_title('classification metrics by model')\n",
                "ax.set_xticks(x + width, classificador)\n",
                "ax.legend(loc='upper left', ncols=3)\n",
                "ax.set_ylim(0,1)\n",
                "\n",
                "plt.show()\n",
                "\n",
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "def grid_RandomForest(X_train, y_train):\n",
                "  model = RandomForestClassifier(random_state=0)\n",
                "  n_estimators = [280,250,290]\n",
                "  criterion = ['gini', 'entropy', 'log_loss']\n",
                "  min_samples_split = [0.12, 0.1, 0.2]\n",
                "  max_depth = [10,12,15]\n",
                "  grid = dict(n_estimators = n_estimators, criterion = criterion,\n",
                "              min_samples_split = min_samples_split, max_depth = max_depth)\n",
                "  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
                "  grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
                "                            scoring='roc_auc',error_score='raise')\n",
                "  grid_result = grid_search.fit(X_train, y_train)\n",
                "  return  grid_result.best_estimator_\n",
                "\n",
                "def grid_Adaboost(X_train, y_train):\n",
                "    model = AdaBoostClassifier(random_state=1)\n",
                "    n_estimators = [2, 15, 35, 50, 70, 100]\n",
                "    learning_rate = np.linspace(0.01, 1, 10)\n",
                "    grid = dict(n_estimators=n_estimators, learning_rate=learning_rate)\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
                "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
                "                               scoring='roc_auc', error_score='raise')\n",
                "    grid_result = grid_search.fit(X_train, y_train)\n",
                "    return grid_result.best_estimator_\n",
                "\n",
                "gridmodelrf = grid_RandomForest(X_train,y_train)\n",
                "y_pred_rf = gridmodelrf.predict(X_test)\n",
                "reportrf = classification_report(y_test,y_pred_rf)\n",
                "\n",
                "gridmodelrf\n",
                "\n",
                "print(reportrf)\n",
                "\n",
                "gridmodelab = grid_Adaboost(X_train,y_train)\n",
                "y_pred_ab = gridmodelab.predict(X_test)\n",
                "reportab = classification_report(y_test,y_pred_ab)\n",
                "\n",
                "print(reportab)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
